import argparse
from dataset import*

def get_args():
    # Tham s·ªë b·∫Øt bu·ªôc nh·∫≠p
    parser = argparse.ArgumentParser(description="Train, Pretrain ho·∫∑c Evaluate m·ªôt model AI")
    parser.add_argument("--epoch", type=int, help="S·ªë epoch ƒë·ªÉ train")
    # parser.add_argument("--model", type=str, required=True, help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn model")
    parser.add_argument("--mode", type=str, choices=["train", "pretrain", "evaluate"], required=True, help="Ch·∫ø ƒë·ªô: train ho·∫∑c pretrain ho·∫∑c evaluate")
    parser.add_argument("--data", type=str, required=True, help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn dataset ƒë√£ gi·∫£i n√©n")
    # Tham s·ªë tr∆∞·ªùng h·ª£p
    parser.add_argument("--checkpoint", type=str, help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file checkpoint (ch·ªâ d√πng cho ch·∫ø ƒë·ªô pretrain)")
    parser.add_argument("--augment", action='store_true', help="B·∫≠t Augmentation cho d·ªØ li·ªáu ƒë·∫ßu v√†o")
    # Tham s·ªë m·∫∑c ƒë·ªãnh(default)
    parser.add_argument("--saveas", type=str, help="Th∆∞ m·ª•c l∆∞u checkpoint")
    parser.add_argument("--lr0", type=float, help="learning rate, default = 0.0001")
    parser.add_argument("--batchsize", type=int, help="Batch size, default = 8")

    parser.add_argument("--weight_decay", type=float,  help="weight_decay, default = 1e-6")
    parser.add_argument("--img_size", type=int, nargs=2,  help="Height and width of the image, default = [256, 256]")
    parser.add_argument("--numclass", type=int, help="shape of class, default = 1")
    parser.add_argument("--warmup", type=int, default=10, help="S·ªë epoch ƒë·ªÉ warm-up (augment nh·∫π)")
    """
    # V·ªõi img_size, c√°ch ch·∫°y: python script.py --img_size 256 256
    N·∫øu mu·ªën nh·∫≠p list d√†i h∆°n 3 ph·∫ßn t·ª≠, g√µ 
    parser.add_argument("--img_size", type=int, nargs='+', default=[256, 256], help="Image dimensions")
    Ch·∫°y:
    python script.py --img_size 128 128 3
    """
    parser.add_argument("--loss", type=str, choices=["Dice_loss", "Hybric_loss", "BCEDice_loss", "BCEwDice_loss", "BCEw_loss", "SoftDice_loss", "Combo_loss", "Tversky_loss", "FocalTversky_loss" ], default="Combo_loss", help="H√†m loss s·ª≠ d·ª•ng, default = Combo_loss")
    parser.add_argument("--optimizer", type=str, choices=["Adam", "SGD", "AdamW"], default="AdamW", help="Optimizer s·ª≠ d·ª•ng, default = AdamW")
    args = parser.parse_args()
    
    # Ki·ªÉm tra logic tham s·ªë
    if args.mode in ["pretrain", "evaluate"] and not args.checkpoint:
        parser.error(f"--checkpoint l√† b·∫Øt bu·ªôc khi mode l√† '{args.mode}'")
        
    return args
def set_seed():
    torch.manual_seed(SEED)
    random.seed(SEED)
    np.random.seed(SEED)
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
# --- [TH√äM H√ÄM N√ÄY] H√ÄM H·ªñ TR·ª¢ ƒê√ìNG/M·ªû BƒÇNG ---
def set_grad_status(model, freeze=True):
    """
    H√†m ƒë√≥ng bƒÉng ho·∫∑c m·ªü bƒÉng Backbone/Encoder.
    H·ªó tr·ª£ c·∫£ Model Custom (self.backbone) v√† Model SMP (self.encoder).
    """
    target_module = None
    
    # 1. Ki·ªÉm tra n·∫øu l√† Model Custom (PyramidCbamGateResNetUNet)
    if hasattr(model, 'backbone'):
        target_module = model.backbone
        name = "Backbone (ResNet)"
    # 2. Ki·ªÉm tra n·∫øu l√† Model SMP (DeepLabV3+, Unet++, ...)
    elif hasattr(model, 'encoder'):
        target_module = model.encoder
        name = "Encoder (SMP)"
    
    if target_module:
        for param in target_module.parameters():
            param.requires_grad = not freeze # Freeze = True -> requires_grad = False
        
        status = "FROZEN ‚ùÑÔ∏è" if freeze else "UNFROZEN üî•"
        print(f"[INFO] {name} is now {status}")
    else:
        print("[WARNING] Could not find 'backbone' or 'encoder' to freeze!")
def model_factory(in_channels=3, num_classes=1):
    return smp.UnetPlusPlus(
        encoder_name="tu-resnest50d", 
        encoder_weights=None, # QUAN TR·ªåNG: ƒê·ªÉ None cho load nhanh, v√¨ ƒë·∫±ng n√†o c≈©ng load checkpoint ƒë√® l√™n
        in_channels=in_channels,
        classes=num_classes,
        drop_path_rate=0.5
    )
def initialize_training_setup(args):
    from utils import get_loss_instance, _focal_tversky_global
    """
    H√†m kh·ªüi t·∫°o d√πng chung cho c·∫£ Train v√† Pretrain ƒë·ªÉ tr√°nh l·∫∑p code.
    Tr·∫£ v·ªÅ: trainer, model, optimizer, criterion, scheduler
    """
    print(f"[INIT] Initializing Model, Optimizer, and Trainer...")
    
    # 1. Kh·ªüi t·∫°o Model
    model = model_factory(in_channels=3, num_classes=1)
    # 2. Load Pretrained Weights (DDSM) n·∫øu c√≥
    # (Logic n√†y d√πng chung cho c·∫£ 2 mode ƒë·ªÅu t·ªët)
    ddsm_checkpoint_path = "best_model_cbis_ddsm.pth"
    if os.path.exists(ddsm_checkpoint_path):
        print(f"[TRANSFER] Loading weights from CBIS-DDSM: {ddsm_checkpoint_path}")
        try:
            state_dict = torch.load(ddsm_checkpoint_path)
            if 'model_state_dict' in state_dict:
                state_dict = state_dict['model_state_dict']
            model.load_state_dict(state_dict)
            print("[TRANSFER] Weights loaded successfully! üöÄ")
        except Exception as e:
            print(f"[ERROR] Weight mismatch: {e}")
    else:
        print(f"[INFO] No DDSM checkpoint found. Training from ImageNet/Scratch.")

    # 3. Optimizer & Scheduler
    opt = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(opt, mode='max', factor=0.5, patience=5, min_lr=1e-6)

    # 4. Loss Function
    criterion = get_loss_instance(args.loss)
    # C·∫≠p nh·∫≠t tham s·ªë n·∫øu l√† FocalTversky
    if args.loss == "FocalTversky_loss":
        alpha, beta, gamma = 0.4, 0.6, 1.33
        _focal_tversky_global.update_params(alpha=alpha, beta=beta, gamma=gamma)
        print(f"[CONFIG] Loss params updated: Alpha={alpha}, Beta={beta}, Gamma={gamma}")

    # 5. Trainer
    return model, opt, criterion, scheduler


def main(args):  
    print(f"\n[DEBUG TRAIN] args.loss b·∫°n nh·∫≠p t·ª´ b√†n ph√≠m = {args.loss}")
    print("-" * 50)
    import numpy as np    
    from trainer import Trainer
    from model import Unet, unet_pyramid_cbam_gate, Swin_unet
    # from model import Swin_unet
    import optimizer as optimizer_module
    from dataset import get_dataloaders
    from result import export, export_evaluate
    global trainer
    from utils import _focal_tversky_global
    from torch.optim.swa_utils import AveragedModel, SWALR, update_bn
    import shutil
    # from utils import loss_func
    from torch.optim.lr_scheduler import _LRScheduler
    print("-" * 50)
    print(f"[INFO] Mode: {args.mode.upper()}")
    print("-" * 50)
    import glob
    import os
    from dataset import EnsembleModel
    set_seed()
    if args.mode == "train":
        if not os.path.exists(BASE_OUTPUT):
            os.makedirs(BASE_OUTPUT)
        # Bi·∫øn l∆∞u k·∫øt qu·∫£ 4 fold
        fold_scores = []
        # --- V√íNG L·∫∂P 4 FOLD ---

        NUM_FOLDS = 4
        for fold_idx in range(NUM_FOLDS):
            print("\n" + "#"*60)
            print(f"### STARTING FOLD {fold_idx + 1}/{NUM_FOLDS} ###")
            print("#"*60 + "\n")
            model, opt, criterion, scheduler = initialize_training_setup(args)
            # trainer = Trainer(model=model, optimizer=opt, criterion=criterion_init, scheduler=scheduler_initial, patience=10, device=DEVICE)
            trainer = Trainer(
                model=model, 
                optimizer=opt, 
                criterion=criterion, 
                scheduler=scheduler, 
                patience=10, 
                device=DEVICE
            )
            # Reset best_val_loss cho fold m·ªõi
            trainer.best_val_loss = float('inf')
            # resume_checkpoint = None
            # --- GIAI ƒêO·∫†N 1: FREEZE ENCODER ---
            # --- B∆Ø·ªöC 1: ƒê√ìNG BƒÇNG ENCODER (ƒê·ªÉ Decoder l√†m quen ·∫£nh m·ªõi tr∆∞·ªõc) ---
            lr_stage1 = 1e-4
            print(f"\n[FOLD {fold_idx}] STEP 1: Freeze Encoder Training")
            print("\n" + "="*40)
            print(" TRANSFER LEARNING: INbreast Dataset")
            print(f" Strategy: Low LR {lr_stage1} + Frozen Encoder First|Train Decoder only")
            print("="*40)
            # [QUAN TR·ªåNG] Augmentation cho INbreast ph·∫£i M·∫†NH v√¨ d·ªØ li·ªáu √≠t
            trainLoader, validLoader, _ = get_dataloaders(aug_mode='strong', state='train', fold_idx=fold_idx)
            set_grad_status(model, freeze=True) # H√†m c√≥ s·∫µn c·ªßa b·∫°n
            trainer.optimizer.param_groups[0]['lr'] = lr_stage1 # Decoder h·ªçc nhanh h∆°n ch√∫t
            trainer.num_epochs = 150 # Ch·∫°y t·∫ßm 20 epoch
            trainer.patience = 30
            trainer.scheduler = None # Kh√¥ng c·∫ßn gi·∫£m LR ƒëo·∫°n n√†y
            # Train nh·∫π
            trainer.train(trainLoader, validLoader, resume_path=None)
            # =========================================================
            # # --- B∆Ø·ªöC 2: M·ªû BƒÇNG TO√ÄN B·ªò (FULL FINE-TUNE) ---
            # =========================================================     
            print(f"\n[FOLD {fold_idx}] STEP 2: Full Fine-tuning, Unfreezing All Layers... Fine-tuning with Low LR.")             
            set_grad_status(model, freeze=False) # M·ªü kh√≥a
            # Update Loss Params (N√™n l√†m m·ªõi m·ªói fold ƒë·ªÉ ch·∫Øc ch·∫Øn)
            if args.loss == "FocalTversky_loss":
                _focal_tversky_global.update_params(alpha=0.4, beta=0.6, gamma=1)
                trainer.best_val_loss = float('inf')
            step1_ckpt = "best_dice_mass_model.pth"
            if os.path.exists(step1_ckpt):
                print(f"[FOLD {fold_idx}] Loading best model from Step 1 manually...")
                trainer.load_checkpoint(step1_ckpt)
            else:
                print(f"[WARNING] No checkpoint found at {step1_ckpt}. Training from scratch/ImageNet.")
            # Reset LR v·ªÅ m·ª©c si√™u th·∫•p
            lr_stage2 = 1e-5 
            weight_decay_stage2 = 1e-2
            trainer.optimizer.param_groups[0]['lr'] = lr_stage2
            trainer.optimizer.param_groups[0]['weight_decay'] = weight_decay_stage2
            print(f"[CONFIG] Updated Optimizer: LR = {lr_stage2} | Weight Decay = {weight_decay_stage2}")
            # G√°n l·∫°i Scheduler ƒë·ªÉ gi·∫£m LR n·∫øu k·∫πt
            trainer.scheduler = scheduler
            
            trainer.num_epochs = NUM_EPOCHS # Ch·∫°y l√¢u
            trainer.patience = 25    # Ki√™n nh·∫´n
            trainer.early_stop_counter = 0 # Reset ƒë·∫øm
            trainer.train(trainLoader, validLoader, resume_path=None)
            # --- L∆ØU K·∫æT QU·∫¢ FOLD ---
            best_dice = trainer.best_dice_mass
            print(f"--> [RESULT] Fold {fold_idx} Best Dice: {best_dice:.4f}")
            fold_scores.append(best_dice)
            # --- XU·∫§T K·∫æT QU·∫¢ (EXPORT) ---
            print(f"\n[INFO] Exporting Fine-tuned Results for FOLD {fold_idx}...")
            # [QUAN TR·ªåNG] G·ªçi h√†m export M·ªöI v·ªõi tham s·ªë fold_idx
            # H√†m n√†y s·∫Ω t·ª± ƒë·ªông t·∫°o folder 'output/fold_X' v√† move file model + csv v√†o ƒë√≥
            export(trainer, fold_idx=fold_idx)
            # =========================================================
            # GIAI ƒêO·∫†N 4: SWA (STOCHASTIC WEIGHT AVERAGING)
            # =========================================================
            # Ch·ªâ ch·∫°y SWA n·∫øu ƒëang d√πng FocalTversky (chi·∫øn l∆∞·ª£c c·ªßa b·∫°n)
            if args.loss == "FocalTversky_loss":
                print("\n" + "="*40)
                print(" GIAI ƒêO·∫†N 4: SWA FINETUNING (The Secret Weapon)")
                print(" Strategy: Constant LR | No Early Stop | 5 Epochs")
                print("="*40)
                # 1. ƒê·ªãnh nghƒ©a th∆∞ m·ª•c Fold hi·ªán t·∫°i
                fold_dir = os.path.join(BASE_OUTPUT, f"fold_{fold_idx}")
                os.makedirs(fold_dir, exist_ok=True)
                # current_fold = fold if 'fold' in locals() else None 
                # 1. QUAN TR·ªåNG: Load l·∫°i BEST MODEL c·ªßa GD3 (Kh√¥ng d√πng model cu·ªëi c√πng)
                # best_model_path = "best_dice_mass_model.pth"
                path_to_best_model = os.path.join(fold_dir, "best_dice_mass_model.pth")
                if not os.path.exists(path_to_best_model):
                    best_ep = trainer.best_epoch_dice
                    best_d = trainer.best_dice_mass
                    folder_name = f"output_epoch{best_ep}_diceMass{best_d:.4f}"
                    path_to_best_model = os.path.join(BASE_OUTPUT, folder_name, "best_dice_mass_model.pth")

                if os.path.exists(path_to_best_model):
                    print(f"[INFO] Loading BEST model from previous stage for SWA: {path_to_best_model}")
                    trainer.load_checkpoint(path_to_best_model)
                else:
                    print(f"[WARNING] Could not find {path_to_best_model}. Using current weights.")

                # 2. Kh·ªüi t·∫°o SWA
                swa_model = AveragedModel(trainer.model)
                # LR cho SWA: Cao h∆°n GD3 m·ªôt ch√∫t ƒë·ªÉ tho√°t h·ªë (5e-5 l√† an to√†n v·ªõi AdamW)
                swa_lr = 5e-5 
                swa_scheduler = SWALR(trainer.optimizer, swa_lr=swa_lr, anneal_epochs=3)
                print(f"[CONFIG] SWA Scheduler set. LR: {swa_lr}")

                # 3. C·∫•u h√¨nh v√≤ng l·∫∑p SWA
                SWA_EPOCHS = 5 # Ch·∫°y c·ªë ƒë·ªãnh
                trainer.patience = 999 # T·∫Øt Early Stop
                trainer.early_stop_counter = 0
                
                # Ch√∫ng ta s·∫Ω d√πng l·∫°i h√†m train() c·ªßa Trainer nh∆∞ng ch·∫°y t·ª´ng epoch m·ªôt
                # ƒë·ªÉ ch√®n logic update_parameters v√†o gi·ªØa.
                
                print("[INFO] Starting SWA Loop...")
                for epoch in range(SWA_EPOCHS):
                    # Hack: Set epoch = 1 ƒë·ªÉ Trainer ch·∫°y 1 v√≤ng r·ªìi tho√°t ra
                    trainer.num_epochs = 1 
                    trainer.start_epoch = 0 
                    # G√°n scheduler SWA v√†o trainer
                    trainer.scheduler = swa_scheduler
                    
                    # Train 1 epoch (Kh√¥ng load checkpoint, ch·∫°y ti·∫øp t·ª´ b·ªô nh·ªõ)
                    # L∆∞u √Ω: Trainer s·∫Ω in ra log validation, c·ª© k·ªá n√≥.
                    print(f"\n[SWA] Epoch {epoch+1}/{SWA_EPOCHS}")
                    trainer.train(trainLoader, validLoader, resume_path=None) # D√πng trainLoader (ƒë√£ strong aug)
                    
                    # C·∫≠p nh·∫≠t tr·ªçng s·ªë trung b√¨nh
                    swa_model.update_parameters(trainer.model)
                    
                    # Step Scheduler
                    swa_scheduler.step()
                    
                # 4. C·∫≠p nh·∫≠t Batch Norm (B∆∞·ªõc b·∫Øt bu·ªôc)
                print("\n[INFO] Updating Batch Normalization statistics for SWA Model...")
                update_bn(trainLoader, swa_model, device=DEVICE)

                # 5. L∆∞u v√† ƒê√°nh gi√° SWA Model
                swa_save_path = os.path.join(fold_dir, "best_model_swa.pth")
                print(f"[INFO] Saving SWA Model to {swa_save_path}")
                swa_checkpoint = {
                    'epoch': SWA_EPOCHS,
                    'model_state_dict': swa_model.state_dict(),         # <--- ƒê√£ s·ª≠a ƒë·ªÉ kh·ªõp t√™n layer
                    'optimizer_state_dict': trainer.optimizer.state_dict(), # ƒê·ªÉ kh√¥ng l·ªói optimizer
                    
                    # C√°c ch·ªâ s·ªë th·ªëng k√™ (L·∫•y t·ª´ trainer hi·ªán t·∫°i ƒë·ªÉ l∆∞u l√†m k·ª∑ ni·ªám)
                    'best_dice_mass': trainer.best_dice_mass,
                }
                torch.save(swa_checkpoint, swa_save_path)
                # export(trainer)
                # ƒê√°nh gi√° Model SWA
                print("\n[INFO] Evaluating SWA Model...")
                # G√°n model SWA v√†o trainer ƒë·ªÉ evaluate
                trainer.model = swa_model
                
                visual_folder = os.path.join(fold_dir, "prediction_images_swa")
                os.makedirs(visual_folder, exist_ok=True)
                
                trainer.evaluate(
                    test_loader=validLoader, 
                    checkpoint_path=swa_save_path,
                    save_visuals=True,          
                    output_dir=visual_folder    
                )
                export_evaluate(trainer, split_name="valid_swa", fold_idx=fold_idx)
        # --- H·∫æT V√íNG FOR (K·∫æT TH√öC 5 FOLD) ---
        # T√≠nh to√°n trung b√¨nh k·∫øt qu·∫£ t·∫°i ƒë√¢y
        if fold_scores:
            mean_score = np.mean(fold_scores)
            std_score = np.std(fold_scores)
            print("\n" + "="*60)
            print(f" FINAL 5-FOLD CV RESULTS")
            print(f" Scores per fold: {fold_scores}")
            print(f" Average Dice: {mean_score:.4f} (+/- {std_score:.4f})")
            print("="*60)    
    # (Gi·ªØ nguy√™n ph·∫ßn pretrain/evaluate)
    elif args.mode == "pretrain":
        print(f"\n[INFO] Mode: PRETRAIN (Single Run)")
        model, opt, criterion, scheduler = initialize_training_setup(args)
        trainer = Trainer(
                model=model, 
                optimizer=opt, 
                criterion=criterion, 
                scheduler=scheduler, 
                patience=20, 
                device=DEVICE
            )
        aug_type = 'strong' if args.augment else 'none'
        trainLoader, validLoader, _ = get_dataloaders(aug_mode=aug_type, state='train', fold_idx=None) 
        trainer.train(trainLoader, validLoader, resume_path=args.checkpoint)
        export(trainer, fold_idx=None) 
    elif args.mode == "evaluate":
        print(f"[INFO] Mode: EVALUATING FULL DATASET")
        
        _, _, testLoader = get_dataloaders(aug_mode='none', state='evaluate')
        model_paths = []
        base_checkpoint_path = args.checkpoint 
        NUM_FOLDS_TO_EVAL = 4
        print(f"[SEARCH] Looking for models in: {base_checkpoint_path}")
        
        for i in range(NUM_FOLDS_TO_EVAL):
            # T√¨m file .pth trong m·ªói fold (d√πng * ƒë·ªÉ b·ªè qua ph·∫ßn t√™n epoch d√†i d√≤ng)
            search_pattern = os.path.join(base_checkpoint_path, f"fold_{i}", "**", "best_dice_mass_model.pth")
            files = glob.glob(search_pattern, recursive=True)
            
            if files:
                model_paths.append(files[0])
                print(f"Fold {i}: Found {files[0]}")
            else:
                print(f"  ! Fold {i}: Warning - File not found at {search_pattern}")
        if len(model_paths) == 0:
            raise ValueError("Kh√¥ng t√¨m th·∫•y model n√†o! Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n output.")
        # 4. KH·ªûI T·∫†O ENSEMBLE MODEL
        print(f"[ENSEMBLE] Initializing Ensemble with {len(model_paths)} models...")
        ensemble_model = EnsembleModel(
            model_class=model_factory,  # Truy·ªÅn h√†m factory
            checkpoint_paths=model_paths, 
            device=DEVICE,
            in_channels=3,
            num_classes=1
        )
        
        trainer = Trainer(model=ensemble_model, device=DEVICE)
        ensemble_output_dir = os.path.join(base_checkpoint_path, "ensemble_predictions_final")
        os.makedirs(ensemble_output_dir, exist_ok=True)
        print(f"[EXEC] Running Inference & Visualization...")
        # G·ªçi h√†m evaluate c√≥ s·∫µn c·ªßa Trainer
        trainer.evaluate(
            test_loader=testLoader, 
            checkpoint_path=None, # Kh√¥ng c·∫ßn load path v√¨ Ensemble ƒë√£ load r·ªìi
            save_visuals=True, 
            output_dir=ensemble_output_dir
        )
        export_evaluate(trainer, split_name="final_ensemble_test", fold_idx="ensemble")
        print("[DONE] Ensemble Evaluation Finished.")

if __name__ == "__main__":
    args = get_args()
    main(args)
